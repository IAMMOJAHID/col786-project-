{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efe3cb4f-eed4-4fc1-9e9e-8b8c1c0875f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.0\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6cde0b-1b42-4495-8159-616c84830f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import monai\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from monai.losses import DiceCELoss \n",
    "from monai.inferers import sliding_window_inference  #input image into model in patch manner\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    EnsureChannelFirstd,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    RandFlipd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    RandShiftIntensityd,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    RandRotate90d,\n",
    "    ResizeWithPadOrCropd,\n",
    "    Lambdad,\n",
    "    SpatialCropd\n",
    ")\n",
    "\n",
    "from monai.config import print_config\n",
    "from monai.metrics import DiceMetric  #to calculate similarity b/w model output and ground truth output\n",
    "from monai.networks.nets import UNETR, SwinUNETR\n",
    "from monai.transforms.spatial.functional import resize\n",
    "\n",
    "from monai.data import (\n",
    "    DataLoader,\n",
    "    CacheDataset,\n",
    "    load_decathlon_datalist,\n",
    "    decollate_batch,\n",
    ")\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82407a5d-b63c-4d1a-b9c5-bdd950d24cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdaa2d5-27ec-4b4c-b09a-168e3ed6a392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_labels(label):\n",
    "    # Set label 1 to foreground and all other labels to background\n",
    "    return torch.where(label == 3, torch.tensor(1), torch.tensor(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27fce8d5-180a-403d-a401-ea873bf6b620",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "monai.transforms.croppad.dictionary CropForegroundd.__init__:allow_smaller: Current default value of argument `allow_smaller=True` has been deprecated since version 1.2. It will be changed to `allow_smaller=False` in version 1.5.\n"
     ]
    }
   ],
   "source": [
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        Spacingd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            pixdim=(1.5, 1.5, 2.0),\n",
    "            mode=(\"bilinear\", \"nearest\"),\n",
    "        ),\n",
    "        ScaleIntensityRanged(\n",
    "            keys=[\"image\"],\n",
    "            a_min=-175,\n",
    "            a_max=250,\n",
    "            b_min=0.0,\n",
    "            b_max=1.0,\n",
    "            clip=True,\n",
    "        ),\n",
    "        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
    "        ResizeWithPadOrCropd(\n",
    "            keys=[\"image\", \"label\"], \n",
    "            spatial_size=(300,250,250),\n",
    "        ),\n",
    "        Lambdad(keys=[\"label\"],func=modify_labels),\n",
    "        RandCropByPosNegLabeld(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            label_key=\"label\",\n",
    "            spatial_size=(96, 96, 96),\n",
    "            pos=1,\n",
    "            neg=1,\n",
    "            num_samples=4,\n",
    "            image_key=\"image\",\n",
    "            image_threshold=0,\n",
    "        ),\n",
    "        RandFlipd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            spatial_axis=[0],\n",
    "            prob=0.10,\n",
    "        ),\n",
    "        RandFlipd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            spatial_axis=[1],\n",
    "            prob=0.10,\n",
    "        ),\n",
    "        RandFlipd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            spatial_axis=[2],\n",
    "            prob=0.10,\n",
    "        ),\n",
    "        RandRotate90d(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            prob=0.10,\n",
    "            max_k=3,\n",
    "        ),\n",
    "        RandShiftIntensityd(\n",
    "            keys=[\"image\"],\n",
    "            offsets=0.10,\n",
    "            prob=0.50,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        Spacingd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            pixdim=(1.5, 1.5, 2.0),\n",
    "            mode=(\"bilinear\", \"nearest\"),\n",
    "        ),\n",
    "        ScaleIntensityRanged(keys=[\"image\"], a_min=-175, a_max=250, b_min=0.0, b_max=1.0, clip=True),\n",
    "        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
    "        ResizeWithPadOrCropd(\n",
    "            keys=[\"image\", \"label\"], \n",
    "            spatial_size=(300,250,250),\n",
    "        ),\n",
    "        Lambdad(keys=[\"label\"],func=modify_labels),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19d2084-c4e7-46c1-b60c-a1d2800d58fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:07<00:00,  1.33s/it]\n",
      "Loading dataset: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:05<00:00,  1.94s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "split_json = \"dataset.json\"\n",
    "datasets = split_json\n",
    "# datasets = \"/kaggle/input/json-file/dataset_0.json\"\n",
    "datalist = load_decathlon_datalist(datasets, True, \"training\")\n",
    "val_files = load_decathlon_datalist(datasets, True, \"validation\")\n",
    "train_ds = CacheDataset(\n",
    "    data=datalist,\n",
    "    transform=train_transforms,\n",
    "    cache_num=6,\n",
    "    cache_rate=1.0,\n",
    "    num_workers=4,\n",
    ")\n",
    "val_ds = CacheDataset(\n",
    "    data=val_files, \n",
    "    transform=val_transforms, \n",
    "    cache_num=3, \n",
    "    cache_rate=1.0, \n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=1, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c8ff4dc-deba-4580-9d61-c7ca2d88c965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: torch.Size([4, 1, 96, 96, 96])\n",
      "label: torch.Size([4, 1, 96, 96, 96])\n",
      "Image: torch.Size([1, 1, 300, 250, 250])\n",
      "label: torch.Size([1, 1, 300, 250, 250])\n"
     ]
    }
   ],
   "source": [
    "for image in enumerate(train_loader):\n",
    "    print(\"Image:\",image[1][\"image\"].shape)\n",
    "    print(\"label:\", image[1][\"label\"].shape)\n",
    "    break\n",
    "\n",
    "for image in enumerate(val_loader):\n",
    "    print(\"Image:\", image[1][\"image\"].shape)\n",
    "    print(\"label:\",image[1][\"label\"].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59e771cb-f960-410d-8188-56b810a5eeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice_map = {\n",
    "#     \"img0035.nii.gz\": 170,\n",
    "#     \"img0036.nii.gz\": 70,\n",
    "#     \"img0037.nii.gz\": 204,\n",
    "#     \"img0038.nii.gz\": 204,\n",
    "#     \"img0039.nii.gz\": 204,\n",
    "#     \"img0040.nii.gz\": 180,\n",
    "# }\n",
    "# case_num = 1\n",
    "# img_name = os.path.split(val_ds[case_num][\"image\"].meta[\"filename_or_obj\"])[1]\n",
    "# img = val_ds[case_num][\"image\"]\n",
    "# label = val_ds[case_num][\"label\"]\n",
    "# img_shape = img.shape\n",
    "# label_shape = label.shape\n",
    "# print(f\"image shape: {img_shape}, label shape: {label_shape}\")\n",
    "# plt.figure(\"image\", (18, 6))\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.title(\"image\")\n",
    "# plt.imshow(img[0, :, :, slice_map[img_name]].detach().cpu(), cmap=\"gray\")\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.title(\"label\")\n",
    "# plt.imshow(label[0, :, :, slice_map[img_name]].detach().cpu())\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b07eaf1-c7a3-4bc5-bea4-fbee95d224be",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device=\"cpu\"\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "\n",
    "model = SwinUNETR(\n",
    "    in_channels=1,\n",
    "    out_channels=2,\n",
    "    img_size=(96, 96, 96),\n",
    "    feature_size=48,\n",
    "    spatial_dims= 3,\n",
    "    dropout_path_rate=0.0,\n",
    "    drop_rate=0.0,\n",
    "    norm_name=\"instance\",\n",
    "    use_checkpoint=True,\n",
    "    attn_drop_rate=0.0,\n",
    ").to(device)\n",
    "\n",
    "loss_function = DiceCELoss(to_onehot_y=True, softmax=True)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6914b5b5-5b02-4ca2-81f4-6a5ed8a2d788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(epoch_iterator_val):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # j=0\n",
    "        for batch in epoch_iterator_val:\n",
    "            val_inputs, val_labels = (batch[\"image\"].cuda(), batch[\"label\"].cuda())\n",
    "            val_outputs = sliding_window_inference(val_inputs, (96, 96, 96), 4, model)\n",
    "            val_labels_list = decollate_batch(val_labels)\n",
    "            val_labels_convert = [post_label(val_label_tensor) for val_label_tensor in val_labels_list]\n",
    "            val_outputs_list = decollate_batch(val_outputs)\n",
    "            val_output_convert = [post_pred(val_pred_tensor) for val_pred_tensor in val_outputs_list]\n",
    "\n",
    "            # for i, tensor in enumerate(val_labels_convert[0]): \n",
    "            #     image_array = np.array(tensor.cpu())\n",
    "            #     nifti_image = nib.Nifti1Image(image_array, affine=None)\n",
    "            #     output_file_path = os.path.join(f'pancreaticModelInputOutput', f'Val_label00{j}_label{i}.nii.gz')\n",
    "            #     nib.save(nifti_image, output_file_path)\n",
    "        \n",
    "                \n",
    "            # print(\"Validation Input Image Saved\")\n",
    "        \n",
    "            # for i, tensor in enumerate(val_output_convert[0]): \n",
    "            #     image_array = np.array(tensor.cpu())\n",
    "            #     nifti_image = nib.Nifti1Image(image_array, affine=None)\n",
    "            #     output_file_path = os.path.join(f'pancreaticModelInputOutput', f'Val_labeloutput00{j}_label{i}.nii.gz')\n",
    "            #     nib.save(nifti_image, output_file_path)\n",
    "                \n",
    "            # print(\"Validation Image Saved\")\n",
    "\n",
    "            # j+=1\n",
    "            \n",
    "            dice_metric(y_pred=val_output_convert, y=val_labels_convert)\n",
    "            epoch_iterator_val.set_description(\"Validate (%d / %d Steps)\" % (global_step, 10.0))  # noqa: B038\n",
    "        mean_dice_val = dice_metric.aggregate().item()\n",
    "        dice_metric.reset()\n",
    "    return mean_dice_val\n",
    "\n",
    "\n",
    "def train(global_step, train_loader, dice_val_best, global_step_best):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    global_step += 1\n",
    "    epoch_iterator = tqdm(train_loader, desc=\"Training (X / X Steps) (loss=X.X)\", dynamic_ncols=True)\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "        step += 1\n",
    "        x, y = (batch[\"image\"].cuda(), batch[\"label\"].cuda())\n",
    "\n",
    "        val_labels_list = decollate_batch(y)\n",
    "        val_labels_convert = [post_label(val_label_tensor) for val_label_tensor in val_labels_list]\n",
    "        logit_map = model(x)\n",
    "        loss = loss_function(logit_map, y)\n",
    "        loss.backward()\n",
    "        epoch_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        epoch_iterator.set_description(  # noqa: B038\n",
    "            \"Training (%d / %d Steps) (loss=%2.5f)\" % (global_step, max_iterations, loss)\n",
    "        )\n",
    "        \n",
    "    if (global_step % eval_num == 0 and global_step != 0) or global_step == max_iterations:\n",
    "            epoch_iterator_val = tqdm(val_loader, desc=\"Validate (X / X Steps) (dice=X.X)\", dynamic_ncols=True)\n",
    "            dice_val = validation(epoch_iterator_val)\n",
    "            print(\"Dice Value:\", dice_val)\n",
    "            epoch_loss /= step\n",
    "            epoch_loss_values.append(epoch_loss)\n",
    "            metric_values.append(dice_val)\n",
    "            if dice_val > dice_val_best:\n",
    "                dice_val_best = dice_val\n",
    "                global_step_best = global_step\n",
    "                torch.save({\n",
    "                        'epoch': global_step,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'loss': loss_function,\n",
    "                        'metric_values': metric_values,\n",
    "                        'loss_values': epoch_loss_values}, \"2Pretrained_SwinUnetr_Pancreatic.pth\")\n",
    "                print(\n",
    "                    \"Model Was Saved ! Current Best Avg. Dice: {} Current Avg. Dice: {}\".format(dice_val_best, dice_val)\n",
    "                )\n",
    "            else:\n",
    "                print(\n",
    "                    \"Model Was Not Saved ! Current Best Avg. Dice: {} Current Avg. Dice: {}\".format(\n",
    "                        dice_val_best, dice_val\n",
    "                    )\n",
    "                )\n",
    "    return global_step, dice_val_best, global_step_best\n",
    "\n",
    "post_label = AsDiscrete(to_onehot=2)\n",
    "post_pred = AsDiscrete(argmax=True, to_onehot=2)\n",
    "dice_metric = DiceMetric(include_background=True, reduction=\"mean\", get_not_nans=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8a9a3a-c2a7-43ff-a94a-799faf46da24",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "dice_val_best = 0.0\n",
    "global_step_best = 0\n",
    "global_step = 3\n",
    "max_iterations = 2500\n",
    "eval_num = 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e9853cc-a5b7-4527-97a6-547646b323b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH = \"best_model.pth\"\n",
    "\n",
    "# checkpoint = torch.load(PATH)\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# epoch = checkpoint['epoch']\n",
    "# criterion = checkpoint['loss']\n",
    "# epoch_loss_values = checkpoint['loss_values']\n",
    "# metric_values = checkpoint['metric_values']\n",
    "# global_step = checkpoint['epoch']\n",
    "\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763ba3d1-cd11-4942-9881-83e8c8a2cd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "while global_step < max_iterations:\n",
    "    global_step, dice_val_best, global_step_best = train(global_step, train_loader, dice_val_best, global_step_best)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aeb2961-e71a-4c8e-9714-229a0eab4a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(\"train\", (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Iteration Average Loss\")\n",
    "x = [eval_num * (i + 1) for i in range(len(epoch_loss_values))]\n",
    "y = epoch_loss_values\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.plot(x, y)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Val Mean Dice\")\n",
    "x = [eval_num * (i + 1) for i in range(len(metric_values))]\n",
    "y = metric_values\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2016195-2ed6-4d34-9e30-94459504b10d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
